# This is a config file for the models available in HarmBench. The entries in this config specify model names, how to load the models, how many GPUs
# to request for the models (e.g., if using SLURM), the model type, and other information.

# ===================== Documentation for the fields ===================== #
# Required fields for all models::
#     model_type: The type of model. This can be "open_source", "closed_source", "open_source_multimodal", or "closed_source_multimodal".
#                 All models are required to have this field in their config. This is used to determine which other fields to expect.

# Required fields for open_source models::
#     model: A dictionary containing arguments to load_model_and_tokenizer in `HarmBench/baselines/model_utils.py`. This is used to load the model
#            and tokenizer. This dictionary is required to have the `model_name_or_path` field. Other fields are optional and can be used to
#            override the default arguments to load_model_and_tokenizer.
#     num_gpus: The number of GPUs to request for the model. This is used to determine the number of GPUs to request when using SLURM.
#               Some methods also load open source LLMs using vLLM or create mini-clusters using ray. For these methods, we use `num_gpus`
#               to allocate the right number of GPUs for the model.
#               NOTE: The default values were chosen to work with 80GB A100 GPUs. If you are using different GPUs, you may need to adjust `num_gpus`.
# 
# Required fields for closed_source models::
#     model: A dictionary containing arguments to load_model_and_tokenizer in `model_utils.py`. This is used to load the model and tokenizer.
#            This dictionary is required to have the following fields, which are handled in `HarmBench/api_models.py`:
#            - model_name_or_path: The name of the API. This is used to select the API class in `HarmBench/api_models.py`.
#            - token: The token for accessing the model
#
# Required fields for open_source_multimodal models::
#     model: A dictionary containing arguments to load_model_and_tokenizer in `HarmBench/baselines/model_utils.py`. This is used to load the model and tokenizer.
# 
# Required fields for closed_source_multimodal models::
#     model: A dictionary containing arguments to load_model_and_tokenizer in `model_utils.py`. This is used to load the model and tokenizer.


# ===================== Open-Source Models ===================== #

# Llama 2


vicuna_13b_v1_5:
  model:
    model_name_or_path: /path/to/inputs/pretrained_models/vicuna-13b-v1.5
    use_fast_tokenizer: False
    dtype: float16
    chat_template: vicuna
  num_gpus: 1
  model_type: open_source

vicuna_13b_v1_5_r2d:
  model:
    model_name_or_path: /path/to/outputs/r2d_outputs/vicuna-13b-v1.5-r2d
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: vicuna
  num_gpus: 1
  model_type: open_source

vicuna_7b_v1_5:
  model:
    model_name_or_path: /path/to/inputs/pretrained_models/vicuna-7b-v1.5
    use_fast_tokenizer: False
    dtype: float16
    chat_template: vicuna
  num_gpus: 1
  model_type: open_source

vicuna_7b_v1_5_r2d:
  model:
    model_name_or_path: /path/to/outputs/r2d_outputs/vicuna-7b-v1.5-r2d
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: vicuna
  num_gpus: 1
  model_type: open_source

qwen_2_7b:
  model:
    model_name_or_path: /path/to/inputs/pretrained_models/Qwen2-7B
    use_fast_tokenizer: True
    dtype: bfloat16
    pad_token: <|endoftext|>
    eos_token: <|im_end|>
    trust_remote_code: True
    chat_template: qwen
  num_gpus: 1
  model_type: open_source

qwen_2_7b_r2d:
  model:
    model_name_or_path: /path/to/outputs/r2d_outputs/Qwen2-7B-r2d
    use_fast_tokenizer: True
    dtype: bfloat16
    pad_token: <|endoftext|>
    eos_token: <|im_end|>
    trust_remote_code: True
    chat_template: qwen
  num_gpus: 1
  model_type: open_source

qwen_2_5_14b:
  model:
    model_name_or_path: /path/to/inputs/pretrained_models/Qwen2.5-14B
    use_fast_tokenizer: True
    dtype: bfloat16
    pad_token: <|endoftext|>
    eos_token: <|im_end|>
    trust_remote_code: True
    chat_template: qwen
  num_gpus: 1
  model_type: open_source

qwen_2_5_14b_r2d:
  model:
    model_name_or_path: /path/to/outputs/r2d_outputs/Qwen2.5-14B-r2d
    use_fast_tokenizer: True
    dtype: bfloat16
    pad_token: <|endoftext|>
    eos_token: <|im_end|>
    trust_remote_code: True
    chat_template: qwen
  num_gpus: 1
  model_type: open_source

mistral_7b_v03:
  model: 
    model_name_or_path: /path/to/inputs/pretrained_models/Mistral-7B-v0.3
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: mistral
  num_gpus: 1
  model_type: open_source


mistral_7b_v03_r2d:
  model: 
    model_name_or_path: /path/to/outputs/r2d_outputs/Mistral-7B-v0.3-r2d
    use_fast_tokenizer: False
    dtype: bfloat16
    chat_template: mistral
  num_gpus: 1
  model_type: open_source

llama3_8b:
  model:
    model_name_or_path: /path/to/inputs/pretrained_models/Meta-Llama-3-8B
    use_fast_tokenizer: False
    dtype: float16
    chat_template: llama-2
  num_gpus: 1
  model_type: open_source

llama3_8b_r2d:
  model:
    model_name_or_path: /path/to/outputs/r2d_outputs/Meta-Llama-3-8B-r2d
    use_fast_tokenizer: False
    dtype: float16
    chat_template: llama-2
  num_gpus: 1
  model_type: open_source
